#!/bin/bash
#RUN using the following command template
#./run_gpu zork <scenario> <agent_type> [GPU_ID] [RANDOM_SEED]

GAME=$1
scenario=$2
agent_tweak=$3                  #"greedy" #"explore" #"merged" #"vanila"
gpu=${4:-0}                     #select scenarion: #4 - 200 objects, troll quest #3 200 objects, egg #2 - 20 objects, short egg quest. #1 - 5 objects, short egg quest
seed=${5:-0}
#####################################################
FRAMEWORK="ZorkFramework"
agent="NeuralQLearner"
netfile="conv_Q_net"            #DQN arch file
obj_net_file="conv_AEN"         #AEN arch file
eval_samples=3                 #number of eval sample steps to save in log (starts to collect)
AEN_n_filters=20
replay_memory=1000000
eval_freq=10000                 # can change ratio between steps and eval_freq to get more samples
eval_steps=2000
prog_freq=10000
save_freq=30000


update_freq=1000
n_replay=1
obj_start=1
#operation modes
bad_parse_penalty_scale=0      #apply penalty multiplier for bad commands
shallow_exploration_flag=0
shallow_elimination_flag=1
double_elimination=1           #use AE predictions to mask invalid actions Q value of the target network

el_beta=1 #0.05
let elimination_freq=$eval_freq*5

obj_bad_cmd_thresh=0.6          #when sigmoid output is larger then the threshold consider it as bad prediction
obj_acc_trust_thresh=0.75       #accuracy threshold for agent to use object net insight
obj_drop_prob=0.7               #probability to drop actions which Obj net condsiders as bad for explore mod
AEN_sample_bias=0.3             #used to increse the number of successful parse samples in AEN buffers
parse_lable_scale=0.001         #from TF guide: A coefficient to use on the positive labels, value < 1 decreases the false positive count and increases the precision.

replay_memory=1000000
max_reward=99
min_reward=-1
discount=0.8
if [ $scenario -gt 4 ]; then
  obj_drop_prob=0.5
  obj_sample=20
  obj_max=100
  cp dqn/131_actions.txt dqn/action_strings.txt

  #cp dqn/1200_sorted_actions.txt dqn/action_strings.txt
  if [ $scenario == 5 ]; then # troll file
    learn_start=200000
       eps_endt=500000
          steps=5000000
  elif [ $scenario == 6 ]; then # open file
    learn_start=50000
       eps_endt=1000000
          steps=2000000
          max_reward=20
          min_reward=0
  elif [ $scenario == 7 ]; then
    obj_sample=20
    obj_max=100
    learn_start=100000
    eps_endt=500000
    steps=2000000
  fi
elif [ $scenario == 4 ]; then # troll 200 take
  obj_sample=5                  #[0,n_objects] how many objects to sample
  obj_max=10                    #[0,n_objects] how many most likely objects to consider with greedy action wp 1
  learn_start=100000
     eps_endt=500000
        steps=5000000
else #egg

  #update_freq=20
  if [ $scenario == 3 ]; then # egg 200 take
    obj_sample=2
    obj_max=5
    learn_start=100000
    eps_endt=200000
    steps=500000
  else # 1 egg 5 take 2: egg 30 take
    obj_sample=1
    obj_max=2
    steps=500000
    learn_start=30000
    eps_endt=200000
  fi
fi
let obj_start=$eval_freq*$obj_start+$learn_start
eps_end=0.05


lr=0.00043                      #DQN learning rate
obj_lr=0.00043                  #AEN learning rate

verbose=2
bufferSize=512                  #512
valid_size=512                  #500
minibatch_size=32               #32
#####################################################

agent_name=$GAME"_scenario_"$scenario"_"$agent_tweak"_Q-arch_"$netfile"_seed_"$seed
if [ $agent_tweak != "vanila" ]; then
  if [ $double_elimination -gt 0 ]; then
    agent_name=$agent_name"_double_elimination"
  fi
  agent_name=$agent_name"_AEN-arch_"$obj_net_file
  if [ $shallow_elimination_flag == 1 ]; then
    agent_name=$agent_name"_SE_beta_"$el_beta"_freq_"$elimination_freq
    obj_max=-1
    obj_sample=0
    if [ $shallow_exploration_flag == 1 ]; then
    agent_name=$agent_name"_SEExplore"
    fi
  fi
  if [ $agent_tweak != "explore" ]; then
    agent_name=$agent_name"_max_"$obj_max"_sample_"$obj_sample
  fi
  if [ $agent_tweak != "greedy" ]; then
    agent_name=$agent_name"_drop_prob_"$obj_drop_prob
  fi
fi
if [ $bad_parse_penalty_scale -gt 0 ]; then
  agent_name=$agent_name"_bp-penalty_"$bad_parse_penalty_scale
fi
env_params="bad_parse_penalty_scale="$bad_parse_penalty_scale",game_scenario="$scenario""
game_path=$PWD"/dqn/zork"

actrep=1
random_starts=0
num_threads=4
pool_frms_type="\"max\""
pool_frms_size=2
pool_frms="type="$pool_frms_type",size="$pool_frms_size
initial_priority="false"
state_rows=65  #@NOTE we added the folowing 2 arguments to allow neural q learner know what are the input dimms
state_cols=300
state_dim=19500 #@DEBUG_DIM(state(sentence)size*word representation)
ncols=1 # number of color channels we only have 1
#nrows=3

agent_params="AEN_n_filters="$AEN_n_filters",shallow_exploration_flag="$shallow_exploration_flag",shallow_elimination_flag="$shallow_elimination_flag",elimination_freq="$elimination_freq",elimination_beta="$el_beta",double_elimination="$double_elimination",AEN_sample_bias="$AEN_sample_bias",parse_lable_scale="$parse_lable_scale",obj_lr="$obj_lr",lr="$lr",ep=1,ep_end="$eps_end",ep_endt="$eps_endt",discount="$discount",hist_len=4,learn_start="$learn_start",replay_memory="$replay_memory",update_freq="$update_freq",n_replay="$n_replay",network="\"$netfile\"",agent_tweak=\"$agent_tweak\",state_dim="$state_dim",minibatch_size="$minibatch_size",rescale_r=1,ncols="$ncols",state_rows="$state_rows",state_cols="$state_cols",bufferSize="$bufferSize",valid_size="$valid_size",target_q=10000,clip_delta=1,min_reward="$min_reward",max_reward="$max_reward",obj_bad_cmd_thresh="$obj_bad_cmd_thresh",obj_start="$obj_start",obj_thresh_acc="$obj_acc_trust_thresh",obj_sample="$obj_sample",obj_max="$obj_max",obj_drop_prob="$obj_drop_prob",obj_net_file="\"$obj_net_file\"""
args="-verbose $verbose -framework $FRAMEWORK -game_path $game_path -name $agent_name -env $GAME -env_params $env_params -agent $agent -agent_params $agent_params -steps $steps -eval_freq $eval_freq -eval_steps $eval_steps -prog_freq $prog_freq -save_freq $save_freq -actrep $actrep -gpu $gpu -random_starts $random_starts -pool_frms $pool_frms -seed $seed -threads $num_threads -eval_samples $eval_samples"
echo $args

cd dqn
qlua train_agent.lua $args
